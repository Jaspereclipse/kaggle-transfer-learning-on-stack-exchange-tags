{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = '../data/train'\n",
    "test_data_dir = '../data/test/test.csv'\n",
    "topics = ['biology', 'cooking', 'crypto', 'diy', 'robotics', 'travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine\n",
    "df_list = []\n",
    "for i, t in enumerate(topics):\n",
    "    df_local = pd.read_csv(osp.join(train_data_dir, \"\".join([t, '.csv'])), encoding='utf-8')\n",
    "    df_local['topic'] = i\n",
    "    df_list.append(df_local)\n",
    "df = pd.concat(df_list)\n",
    "df[\"Qid\"] = df[\"topic\"].map(str) + \"-\" + df[\"id\"].map(str)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop([\"index\", \"id\", \"topic\"], 1, inplace=True)\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>Qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86995</th>\n",
       "      <td>Tipping in USA.California.SF</td>\n",
       "      <td>&lt;p&gt;Being a &lt;a href=\"https://gsamaras.wordpress...</td>\n",
       "      <td>usa food-and-drink california tipping san-fran...</td>\n",
       "      <td>5-78013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86996</th>\n",
       "      <td>As a Canadian, what are appropriate gifts to b...</td>\n",
       "      <td>&lt;p&gt;I live in Vancouver, Canada and will be tra...</td>\n",
       "      <td>uk canada france culture gifts</td>\n",
       "      <td>5-78016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86997</th>\n",
       "      <td>Does Macedonian police issue visitors a regist...</td>\n",
       "      <td>&lt;p&gt;In Serbia and Macedonia, you have to Regist...</td>\n",
       "      <td>customs-and-immigration officials registration...</td>\n",
       "      <td>5-78018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86998</th>\n",
       "      <td>Can my Austrian Visa D be renewed?</td>\n",
       "      <td>&lt;p&gt;I am in Austria on a Visa D multiple entry,...</td>\n",
       "      <td>visas austria</td>\n",
       "      <td>5-78019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86999</th>\n",
       "      <td>Shortest wide-body route from Frankfurt</td>\n",
       "      <td>&lt;p&gt;What is the shortest passenger route served...</td>\n",
       "      <td>untagged</td>\n",
       "      <td>5-78022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "86995                       Tipping in USA.California.SF   \n",
       "86996  As a Canadian, what are appropriate gifts to b...   \n",
       "86997  Does Macedonian police issue visitors a regist...   \n",
       "86998                 Can my Austrian Visa D be renewed?   \n",
       "86999            Shortest wide-body route from Frankfurt   \n",
       "\n",
       "                                                 content  \\\n",
       "86995  <p>Being a <a href=\"https://gsamaras.wordpress...   \n",
       "86996  <p>I live in Vancouver, Canada and will be tra...   \n",
       "86997  <p>In Serbia and Macedonia, you have to Regist...   \n",
       "86998  <p>I am in Austria on a Visa D multiple entry,...   \n",
       "86999  <p>What is the shortest passenger route served...   \n",
       "\n",
       "                                                    tags      Qid  \n",
       "86995  usa food-and-drink california tipping san-fran...  5-78013  \n",
       "86996                     uk canada france culture gifts  5-78016  \n",
       "86997  customs-and-immigration officials registration...  5-78018  \n",
       "86998                                      visas austria  5-78019  \n",
       "86999                                           untagged  5-78022  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly look into one row\n",
    "def peek():\n",
    "    df_slice = df.sample(1)\n",
    "    for col in df_slice.columns.values:\n",
    "        print list(df_slice[col])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat6/Low Voltage Termination in a wooden enclosure\n",
      "<p>I have been bouncing around the idea of running CAT6 thru my older (1970s) house.  Single story with an unfinished attic</p>\n",
      "\n",
      "<p>I have identified a bedroom closet as the potential location for the central wiring location.  The closet is on the outer wall of the house.</p>\n",
      "\n",
      "<p>Rather than install a patch panel in the closet itself and have to deal with tidying up all of the resulting cables, I was toying with the idea of a wooden enclosure directly above the closet</p>\n",
      "\n",
      "<p>Something along the lines of this\n",
      "<a href=\"http://www.ikea.com/us/en/catalog/products/S89046815/\" rel=\"nofollow\">http://www.ikea.com/us/en/catalog/products/S89046815/</a>\n",
      "With a pair of 6U rails to mount the patch panels.  Rails would be mounted about 6\" in on the client cable side and 9\" in on the termination side</p>\n",
      "\n",
      "<p>Nail some attic decking onto the joists and secure the enclosure onto the decking</p>\n",
      "\n",
      "<p>Run all cables up into the attic\n",
      "Drill a 4\" hole on the side of the enclosure to feed cables from wall jacks (or maybe drill a series of holes and just run smurf tube directly into the enclosure)\n",
      "Drill a 3\" hole on the bottom and run a short length of PVC to feed the switch (24 port for now but possibly 48 later) or pass patch cables from router and VOIP up to the panel.  </p>\n",
      "\n",
      "<p>Would this raise any code violations?<br>\n",
      "Potential fire hazard maybe from terminating inside a wooden enclosure in the unfinished attic?\n",
      "Any structural code issues from putting this near the outer wall/studs.</p>\n",
      "\n",
      "<p>Any other problems this might cause that I'm overlooking?</p>\n",
      "\n",
      "electrical code-compliance attic data-wiring\n",
      "3-77029\n"
     ]
    }
   ],
   "source": [
    "peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process(data=df):\n",
    "    #1: remove html tags\n",
    "    removeHtmlTags = lambda x: BeautifulSoup(x, \"html.parser\").text\n",
    "    data[\"content_cleaned\"] = data['content'].apply(removeHtmlTags)\n",
    "    print \"Removed html tags.\"\n",
    "    #2: combine title and content\n",
    "    tmp = data['title'] + \" \" + data['content_cleaned']\n",
    "    print \"Combined question title and content.\"\n",
    "    #3: remove new line, extra space\n",
    "    clean = lambda x: re.sub(\"[ ]+\", \" \", x.strip(\"\\n\"))\n",
    "    data[\"combined_text\"] = tmp.apply(clean)\n",
    "    print \"Removed spaces and new lines.\"\n",
    "    keeps = ['Qid', 'combined_text', 'tags']\n",
    "    return data[keeps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed html tags.\n",
      "Combined question title and content.\n",
      "Removed spaces and new lines.\n"
     ]
    }
   ],
   "source": [
    "df = pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-57585\n",
      "Non-EU citizen Schengen visa via NYC Swiss embassy I am applying for Schengen visa via Switzerland embassy in NYC.\n",
      "One of their requirements is :\n",
      "\n",
      "A copy of your confirmed flight reservation.\n",
      "\n",
      "How do I purchase temporary tickets to meet this requirement?\n",
      "visas schengen\n"
     ]
    }
   ],
   "source": [
    "peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import cPickle as pkl\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_word_dict(df, limit=None):\n",
    "    corpus = df[\"combined_text\"].str.cat(sep=\" \")\n",
    "    corpus = re.sub(\"[ ]+\", \" \", corpus) # one gigantic text\n",
    "    words =  word_tokenize(corpus, 'english')\n",
    "    if limit is None:\n",
    "        limit = len(set(words))\n",
    "    vc = {}\n",
    "    vc['UNKNOWN'] = 0\n",
    "    for word,_ in FreqDist(words).most_common(limit):\n",
    "        vc[word] = len(vc) # might overwrite 'UNKNOWN'\n",
    "    vc['NULL'] = len(vc)\n",
    "    vc['UNKNOWN'] = 0\n",
    "    rv_vc = {v:k for k, v in vc.iteritems()} # reverse volcabulary\n",
    "    return vc, rv_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2id, id2word = build_word_dict(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212590"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict_file = \"../data/tmp/word2id.pkl\"\n",
    "reversed_word_dict_file = \"../data/tmp/id2word.pkl\"\n",
    "with open(word_dict_file, \"wb\") as f:\n",
    "    pkl.dump(word2id, f)\n",
    "with open(reversed_word_dict_file, \"wb\") as f:\n",
    "    pkl.dump(id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict_file = \"../data/tmp/word2id.pkl\"\n",
    "reversed_word_dict_file = \"../data/tmp/id2word.pkl\"\n",
    "if osp.exists(word_dict_file):\n",
    "    with open(word_dict_file, \"rb\") as f:\n",
    "        word2id = pkl.load(f)\n",
    "if osp.exists(reversed_word_dict_file):\n",
    "    with open(reversed_word_dict_file, \"rb\") as f:\n",
    "        id2word = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(df, vocab): \n",
    "    ssplit = lambda row: sent_tokenize(row[\"combined_text\"], \"english\")\n",
    "    gendf = lambda sents, row: pd.DataFrame(zip([row[\"Qid\"]]*len(sents), sents), columns=[\"Qid\", \"Sentence\"])\n",
    "    first = df.loc[0, :]\n",
    "    sents = ssplit(first)\n",
    "    df_sent = gendf(sents, first)\n",
    "    nrow = len(df.index)\n",
    "    disp = nrow / 10\n",
    "    for idx in xrange(1, nrow):\n",
    "        row = df.loc[idx, :]\n",
    "        sents = ssplit(row)\n",
    "        df_sent = df_sent.append(gendf(sents, row), ignore_index=True)\n",
    "        if (idx+1) % disp == 0:\n",
    "            print \"Processed: %d\"%(idx+1)\n",
    "    text2numeric = lambda s: [vocab[w] if w in vocab else 0 for w in word_tokenize(s, \"english\")]\n",
    "    df_sent[\"Encoding\"] = df_sent[\"Sentence\"].apply(text2numeric)\n",
    "    df_sent[\"Length\"] = df_sent[\"Encoding\"].apply(len)\n",
    "    df_sent = df_sent.loc[df_sent[\"Length\"] > 3, :]\n",
    "    df_sent = df_sent.loc[df_sent[\"Length\"] < 50, :]\n",
    "    return df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 8700\n",
      "Processed: 17400\n",
      "Processed: 26100\n",
      "Processed: 34800\n",
      "Processed: 43500\n",
      "Processed: 52200\n",
      "Processed: 60900\n",
      "Processed: 69600\n",
      "Processed: 78300\n",
      "Processed: 87000\n"
     ]
    }
   ],
   "source": [
    "df_sent = build_dataset(df, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_file = \"../data/tmp/sentences.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sent.to_csv(sents_file, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sent = pd.read_csv(sents_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86632\n"
     ]
    }
   ],
   "source": [
    "print \"Number of questions: %d\" %len(set(df_sent['Qid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(df, batch_size, window=5, null_id=word2id[\"NULL\"]): # use paragraph and left N-1 words to predict the N-th word\n",
    "    batch = np.ndarray(shape=(batch_size, 5), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    sample = df.sample(n=batch_size//10).reset_index()\n",
    "    assert sample[\"Length\"].sum() >= batch_size, \"Not enought data: %d (< %d)\" %(sample[\"Length\"].sum(), batch_size)\n",
    "    cnt = 0 # fill rate\n",
    "    row = 0\n",
    "    while cnt < batch_size:\n",
    "        buffer_ = [null_id] * window + sample.loc[row, \"Encoding\"] # paddings \n",
    "        for i in xrange(len(buffer_) - window):\n",
    "            if cnt < batch_size:\n",
    "                batch[cnt, :] = buffer_[i: i + window]\n",
    "                labels[cnt, 0] = buffer_[i + window]\n",
    "                cnt += 1\n",
    "            else:\n",
    "                break\n",
    "        row += 1\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch, labels = get_batch(df_sent, 128, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-requisites:\n",
    "# 1. word2id\n",
    "# 2. id2word\n",
    "# 3. df_sent: [\"Qid\", \"Sentence\", \"Encoding\", \"Length\"]\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string(\"train_data\", None, \"Doc2Vec training data\")\n",
    "flags.DEFINE_integer(\"doc_embedding_size\", 200, \"embedding size for paragraph\")\n",
    "flags.DEFINE_integer(\"word_embedding_size\", 100, \"embedding size for word\")\n",
    "flags.DEFINE_integer(\"vocab_size\", len(word2id), \"vocabulary size\")\n",
    "flags.DEFINE_integer(\"num_paragraphs\", len(set(df_sent['Qid'])), \"number of paragraphs in the training set\")\n",
    "flags.DEFINE_boolean(\"concat\", True, \"whether to concatenate paragraph embedding with word embeddings\")\n",
    "flags.DEFINE_boolean(\"sum_\", False, \"whether to sum all embeddings (both paragraph and word)\")\n",
    "flags.DEFINE_boolean(\"average\", False, \"whether to average all embeddings (both paragraph and word)\")\n",
    "flags.DEFINE_integer(\"window\", 4, \"number of preceding words used to predict next word\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    \"\"\"\n",
    "    Options used by doc2vec model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = FLAGS.train_data\n",
    "        self.wd_emb_dim = FLAGS.word_embedding_size\n",
    "        self.ph_emb_dim = FLAGS.doc_embedding_size\n",
    "        self.vocab_size = FLAGS.vocab_size\n",
    "        self.num_paragraphs = FLAGS.num_paragraphs\n",
    "        self.concat = FLAGS.concat\n",
    "        self.sum_ = FLAGS.sum_\n",
    "        self.average = FLAGS.average\n",
    "        assert self.concat or self.sum_ or self.average, \"You either concat or sum/average input embeddings\"\n",
    "        if not self.concat:\n",
    "            assert self.sum_ or self.average, \"You either sum or average input embeddings\"\n",
    "            assert self.wd_emb_dim == self.ph_emb_dim, \n",
    "            \"If not concatenated, paragraph embeddings should have the same size as word embedding's\"\n",
    "        self.window = FLAGS.window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Doc2Vec(object):\n",
    "    \"\"\"\n",
    "    Distributed Memory Model of Paragraph Vectors (PV-DM).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, options, session):\n",
    "        self._options = options\n",
    "        self._session = session\n",
    "        self._wd_embed = None\n",
    "        self._ph_embed = None\n",
    "        self.global_step = None\n",
    "    \n",
    "    def forward(self, batch_data, batch_labels):\n",
    "        opts = self._options\n",
    "        \n",
    "        # Word and paragraph embeddings\n",
    "        wd_embed_init_width = 0.5 / opts.wd_emb_dim\n",
    "        ph_embed_init_width = 0.5 / opts.ph_emb_dim\n",
    "        wd_embed = tf.Variable(tf.random_uniform([opts.vocab_size, opts.wd_embed_dim], -wd_embed_init_width, \n",
    "                              wd_embed_init_width), name=\"word_embedding\")\n",
    "        ph_embed = tf.Variable(tf.random_uniform([opts.num_paragraphs, opts.ph_embed_din], -ph_embed_init_width,\n",
    "                                                ph_embed_init_width), name=\"paragraph_embedding\")\n",
    "        self._wd_embed = wd_embed\n",
    "        self._ph_embed = ph_embed\n",
    "        \n",
    "        # Softmax weight & biases\n",
    "        if opts.concat:\n",
    "            tf.sm_wgt = tf.Variable(tf.zeros([opts.vocab_size, opts.ph_emb_dim + opts.window * opts.wd_emb_dim]), \n",
    "                                   name=\"Softmax weights\")\n",
    "        else:\n",
    "            tf.sm_wgt = tf.Variable(tf.zeros([opts.vocab_size, opts.wd_emb_dim]), name=\"Softmax weights\")\n",
    "        tf.sm_b = tf.Variable(tf.zeros([opts.vocab_size]), name=\"Softmax biases\")\n",
    "        \n",
    "        # Global step\n",
    "        self.global_step = tf.Variable(0, \"global step\")\n",
    "         \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
